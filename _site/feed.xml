<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.1.1">Jekyll</generator><link href="http://sdaza.com/feed.xml" rel="self" type="application/atom+xml" /><link href="http://sdaza.com/" rel="alternate" type="text/html" /><updated>2016-08-30T13:06:51-05:00</updated><id>http://sdaza.com/</id><title>Sebastian Daza</title><subtitle>Sebastian Daza Blog</subtitle><entry><title>R package to compute statistics from the American Community Survey</title><link href="http://sdaza.com/r/2016/07/06/acsr/" rel="alternate" type="text/html" title="R package to compute statistics from the American Community Survey" /><published>2016-07-06T00:00:00-05:00</published><updated>2016-07-06T00:00:00-05:00</updated><id>http://sdaza.com/r/2016/07/06/acsr</id><content type="html" xml:base="http://sdaza.com/r/2016/07/06/acsr/">The `acsr` package is to extract variables and compute statistics using the America Community Survey in a *systematic way*. It was created for the [Applied Population Laboratory](http://www.apl.wisc.edu/) (APT) at the University of Wisconsin-Madison.

&lt;h2 class=&quot;section-heading&quot;&gt;Installation&lt;/h2&gt;

The functions depend on the `acs` and `data.table` packages, so it is necessary to install then before using `acsr`. The `acsr` package is hosted on a github repository and can be installed using `devtools`:


{% highlight r %}
devtools::install_github(&quot;sdaza/acsr&quot;)
library(acsr)
{% endhighlight %}



Remember to set the ACS API key, in case you haven&#39;t, and to check the help documentation and the default values of the `acsr` functions.


{% highlight r %}
api.key.install(key=&quot;*&quot;)
?sumacs
?acsdata
{% endhighlight %}

The default level is `state`, specifically the state of Wisconsin (`state = &quot;WI&quot;`), the `endyear` is 2014, and the confidence level to compute margins of error (MOEs) is 90%.

&lt;h2 class=&quot;section-heading&quot;&gt;Levels&lt;/h2&gt;

The `acsr` functions can extract all the levels available in the `acs` package. The table below shows the summary and required levels when using the `acsdata` and `sumacs` functions:

| summary number    | levels                                    |
|----------------   |-----------------------------------------  |
| 010               | us                                        |
| 020               | region                                    |
| 030               | division                                  |
| 040               | state                                     |
| 050               | state, county                             |
| 060               | state, county, county.subdivision         |
| 140               | state, county, tract                      |
| 150               | state, county, tract, block.group         |
| 160               | state, place                              |
| 250               | american.indian.area                      |
| 320               | state, msa                                |
| 340               | state, csa                                |
| 350               | necta                                     |
| 400               | urban.area                                |
| 500               | state, congressional.district             |
| 610               | state, state.legislative.district.upper   |
| 620               | state, state.legislative.district.lower   |
| 795               | state, puma                               |
| 860               | zip.code                                  |
| 950               | state, school.district.elementary         |
| 960               | state, school.district.secondary          |
| 970               | state, school.district.unified            |


&lt;h2 class=&quot;section-heading&quot;&gt;Getting variables and statistics&lt;/h2&gt;

We can use the `sumacs` function to extract variable and statistics. We have to specify the corresponding method (e.g., *proportion* or just *variable*), and the name of the statistic or variable to be included in the output.


{% highlight r %}
sumacs(formula = c(&quot;(b16004_004 + b16004_026 + b16004_048 / b16004_001)&quot;, &quot;b16004_026&quot;),
        varname = c&quot;mynewvar&quot;, &quot;myvar&quot;),
        method = c(&quot;prop&quot;, &quot;variable&quot;),
        level = c(&quot;division&quot;))
{% endhighlight %}



{% highlight text %}
## Error: &lt;text&gt;:2:20: unexpected string constant
## 1: sumacs(formula = c(&quot;(b16004_004 + b16004_026 + b16004_048 / b16004_001)&quot;, &quot;b16004_026&quot;),
## 2:         varname = c&quot;mynewvar&quot;
##                       ^
{% endhighlight %}

To download the data can be slow, especially when many levels are being used (e.g., blockgroup). A better approach in those case is to download the data first using the function `acsdata`, and then use them as input.



{% highlight r %}
mydata &lt;- acsdata(formula = c(&quot;(b16004_004 + b16004_026 + b16004_048 /  b16004_001)&quot;,
        &quot;b16004_026&quot;),
        level = c(&quot;division&quot;))

sumacs(formula = c(&quot;(b16004_004 + b16004_026 + b16004_048 / b16004_001)&quot;, &quot;b16004_026&quot;),
        varname = c&quot;mynewvar&quot;, &quot;myvar&quot;),
        method = c(&quot;prop&quot;, &quot;variable&quot;),
        level = c(&quot;division&quot;),
        data = mydata)
{% endhighlight %}



{% highlight text %}
## Error: &lt;text&gt;:6:20: unexpected string constant
## 5: sumacs(formula = c(&quot;(b16004_004 + b16004_026 + b16004_048 / b16004_001)&quot;, &quot;b16004_026&quot;),
## 6:         varname = c&quot;mynewvar&quot;
##                       ^
{% endhighlight %}

&lt;h2 class=&quot;section-heading&quot;&gt;Standard errors&lt;/h2&gt;

When computing statistics there are two ways to define the standard errors:

- Including all standard errors of the variables used to compute a statistic (`one.zero = FALSE`)
- Include all standard errors except those of variables that are equal to zero. Only the maximum standard error of the variables equal to zero is included  (`one.zero = TRUE`)
- The default value is `one.zero = TRUE`

For more details about how standard errors are computed for proportions, ratios and aggregations look at [A Compass for Understanding and Using American Community Survey Data](https://www.census.gov/content/dam/Census/library/publications/2008/acs/ACSGeneralHandbook.pdf).

Below an example when estimating proportions and using `one.zero = FALSE`:


{% highlight r %}
sumacs(formula = &quot;(b16004_004 + b16004_026 + b16004_048) / b16004_001&quot;,
            varname =&quot;mynewvar&quot;,
            method = &quot;prop&quot;,
            level = &quot;tract&quot;,
            county = 1,
            tract = 950501,
            endyear = 2013,
            one.zero = FALSE)
{% endhighlight %}



{% highlight text %}
## [1] &quot;. . . . . .  ACS variables : 4&quot;
## [1] &quot;. . . . . .  Levels : 1&quot;
## [1] &quot;. . . . . .  New variables : 1&quot;
## [1] &quot;. . . . . .  Getting tract data&quot;
## [1] &quot;. . . . . .  Done!&quot;
## [1] &quot;. . . . . .  Creating variables&quot;
## [1] &quot;. . . . . .  100%&quot;
## [1] &quot;. . . . . .  Formatting output&quot;
## [1] &quot;. . . . . .  Done!&quot;
{% endhighlight %}



{% highlight text %}
##    sumlevel       geoid st_fips cnty_fips tract_fips mynewvar_est mynewvar_moe
## 1:      140 55001950501      55         1     950501       0.0226       0.0252
{% endhighlight %}

$$ SE = \sqrt{ \frac{(5.47 ^ 2 + 22.49 ^ 2 + 5.47 ^ 2) - ( 0.02 ^ 2 \times 102.13 ^ 2)}{1546} } \times 1.645 = 0.0252 $$

When `one.zero = TRUE`:


{% highlight r %}
sumacs(formula = &quot;(b16004_004 + b16004_026 + b16004_048) / b16004_001&quot;,
            varname =&quot;mynewvar&quot;,
            method = &quot;prop&quot;,
            level = &quot;tract&quot;,
            county = 1,
            tract = 950501,
            endyear = 2013,
            one.zero = TRUE)
{% endhighlight %}



{% highlight text %}
## [1] &quot;. . . . . .  ACS variables : 4&quot;
## [1] &quot;. . . . . .  Levels : 1&quot;
## [1] &quot;. . . . . .  New variables : 1&quot;
## [1] &quot;. . . . . .  Getting tract data&quot;
## [1] &quot;. . . . . .  Done!&quot;
## [1] &quot;. . . . . .  Creating variables&quot;
## [1] &quot;. . . . . .  100%&quot;
## [1] &quot;. . . . . .  Formatting output&quot;
## [1] &quot;. . . . . .  Done!&quot;
{% endhighlight %}



{% highlight text %}
##    sumlevel       geoid st_fips cnty_fips tract_fips mynewvar_est mynewvar_moe
## 1:      140 55001950501      55         1     950501       0.0226       0.0245
{% endhighlight %}

$$ SE_{\text{ one.zero}} \sqrt{ \frac{(5.47 ^ 2 + 22.49 ^ 2) - ( 0.02 ^ 2  \times 102.13 ^ 2)}{1546} }  \times 1.645 = 0.0245 $$

When the square root value in the standard error formula doesn&#39;t exist (e.g., the square root of a negative number), the ratio formula is instead used. The ratio adjustment is done **variable by variable** .

It can  also be that the `one.zero` option makes the square root undefinable. In those cases, the function uses again the **ratio** formula to compute standard errors. There is also a possibility that the standard error estimates using the **ratio** formula are higher than the **proportion** estimates without the `one.zero` option.

&lt;h2 class=&quot;section-heading&quot;&gt;Output&lt;/h2&gt;

The output can be formatted using a wide or long format:


{% highlight r %}
sumacs(formula = &quot;(b16004_004 + b16004_026 + b16004_048 / b16004_001)&quot;,
            varname =&quot;mynewvar&quot;,
            method = &quot;prop&quot;,
            level = &quot;division&quot;,
            format.out = &quot;long&quot;)
{% endhighlight %}



{% highlight text %}
## [1] &quot;. . . . . .  ACS variables : 4&quot;
## [1] &quot;. . . . . .  Levels : 1&quot;
## [1] &quot;. . . . . .  New variables : 1&quot;
## [1] &quot;. . . . . .  Getting division data&quot;
## [1] &quot;. . . . . .  Done!&quot;
## [1] &quot;. . . . . .  Creating variables&quot;
## [1] &quot;. . . . . .  100%&quot;
## [1] &quot;. . . . . .  Formatting output&quot;
## [1] &quot;. . . . . .  Done!&quot;
{% endhighlight %}



{% highlight text %}
##    geoid sumlevel division var_name    est      moe
## 1:    NA      030        1 mynewvar 0.0762 0.000347
## 2:    NA      030        2 mynewvar 0.1182 0.000278
## 3:    NA      030        3 mynewvar 0.0599 0.000196
## 4:    NA      030        4 mynewvar 0.0411 0.000277
## 5:    NA      030        5 mynewvar 0.1108 0.000246
## 6:    NA      030        6 mynewvar 0.0320 0.000265
## 7:    NA      030        7 mynewvar 0.2203 0.000469
## 8:    NA      030        8 mynewvar 0.1582 0.000602
## 9:    NA      030        9 mynewvar 0.2335 0.000501
{% endhighlight %}

And it can also be exported to a csv file:


{% highlight r %}
sumacs(formula = &quot;(b16004_004 + b16004_026 + b16004_048 / b16004_001)&quot;,
            varname =&quot;mynewvar&quot;,
            method = &quot;prop&quot;,
            level = &quot;division&quot;,
            file = &quot;myfile.out&quot;)
{% endhighlight %}



{% highlight text %}
## [1] &quot;. . . . . .  ACS variables : 4&quot;
## [1] &quot;. . . . . .  Levels : 1&quot;
## [1] &quot;. . . . . .  New variables : 1&quot;
## [1] &quot;. . . . . .  Getting division data&quot;
## [1] &quot;. . . . . .  Done!&quot;
## [1] &quot;. . . . . .  Creating variables&quot;
## [1] &quot;. . . . . .  100%&quot;
## [1] &quot;. . . . . .  Formatting output&quot;
## [1] &quot;. . . . . .  Data exported to a CSV file! Done!&quot;
{% endhighlight %}

&lt;h2 class=&quot;section-heading&quot;&gt;Combining geographic levels&lt;/h2&gt;

We can combine geographic levels using two methods: (1) `sumacs` and (2) `combine.output`. The first one allows only single combinations, the second multiple ones.

If I want to combine two states (e.g., Wisconsin and Minnesota) I can use:


{% highlight r %}
sumacs(&quot;(b16004_004 + b16004_026 + b16004_048 / b16004_001)&quot;,
    varname = &quot;mynewvar&quot;,
    method = &quot;prop&quot;,
    level = &quot;state&quot;,
    state = list(&quot;WI&quot;, &quot;MN&quot;),
    combine = TRUE)
{% endhighlight %}



{% highlight text %}
## [1] &quot;. . . . . .  ACS variables : 4&quot;
## [1] &quot;. . . . . .  Levels : 1&quot;
## [1] &quot;. . . . . .  New variables : 1&quot;
## [1] &quot;. . . . . .  Getting combined data&quot;
## [1] &quot;. . . . . .  Printing geographic levels&quot;
## [[1]]
## &quot;geo&quot; object: [1] &quot;Wisconsin&quot;
## 
## [[2]]
## &quot;geo&quot; object: [1] &quot;Minnesota&quot;
## 
## [1] &quot;. . . . . .  .  .  .  .  .  .  .  .  .  .&quot;
## [1] &quot;. . . . . .  Done!&quot;
## [1] &quot;. . . . . .  Creating variables&quot;
## [1] &quot;. . . . . .  100%&quot;
## [1] &quot;. . . . . .  Formatting output&quot;
## [1] &quot;. . . . . .  Done!&quot;
{% endhighlight %}



{% highlight text %}
##    geoid combined_group mynewvar_est mynewvar_moe
## 1:    NA      aggregate        0.042     0.000331
{% endhighlight %}

If I want to put together multiple combinations (e.g., groups of states):


{% highlight r %}
combine.output(&quot;(b16004_004 + b16004_026 + b16004_048 / b16004_001)&quot;,
    varname = &quot;mynewvar&quot;,
    method = &quot;prop&quot;,
    level = list(&quot;state&quot;, &quot;state&quot;),
    state = list( list(&quot;WI&quot;, &quot;MN&quot;), list(&quot;CA&quot;, &quot;OR&quot;)), # nested list
    combine.names = c(&quot;WI+MN&quot;, &quot;CA+OR&quot;))
{% endhighlight %}



{% highlight text %}
## [1] &quot;. . . . . .  Defining WI+MN&quot;
## [1] &quot;. . . . . .  ACS variables : 4&quot;
## [1] &quot;. . . . . .  Levels : 1&quot;
## [1] &quot;. . . . . .  New variables : 1&quot;
## [1] &quot;. . . . . .  Getting combined data&quot;
## [1] &quot;. . . . . .  Printing geographic levels&quot;
## [[1]]
## &quot;geo&quot; object: [1] &quot;Wisconsin&quot;
## 
## [[2]]
## &quot;geo&quot; object: [1] &quot;Minnesota&quot;
## 
## [1] &quot;. . . . . .  .  .  .  .  .  .  .  .  .  .&quot;
## [1] &quot;. . . . . .  Done!&quot;
## [1] &quot;. . . . . .  Creating variables&quot;
## [1] &quot;. . . . . .  100%&quot;
## [1] &quot;. . . . . .  Formatting output&quot;
## [1] &quot;. . . . . .  Done!&quot;
## [1] &quot;. . . . . .  Defining CA+OR&quot;
## [1] &quot;. . . . . .  ACS variables : 4&quot;
## [1] &quot;. . . . . .  Levels : 1&quot;
## [1] &quot;. . . . . .  New variables : 1&quot;
## [1] &quot;. . . . . .  Getting combined data&quot;
## [1] &quot;. . . . . .  Printing geographic levels&quot;
## [[1]]
## &quot;geo&quot; object: [1] &quot;California&quot;
## 
## [[2]]
## &quot;geo&quot; object: [1] &quot;Oregon&quot;
## 
## [1] &quot;. . . . . .  .  .  .  .  .  .  .  .  .  .&quot;
## [1] &quot;. . . . . .  Done!&quot;
## [1] &quot;. . . . . .  Creating variables&quot;
## [1] &quot;. . . . . .  100%&quot;
## [1] &quot;. . . . . .  Formatting output&quot;
## [1] &quot;. . . . . .  Done!&quot;
{% endhighlight %}



{% highlight text %}
##    combined_group mynewvar_est mynewvar_moe
## 1:          WI+MN        0.042     0.000331
## 2:          CA+OR        0.269     0.000565
{% endhighlight %}


&lt;h2 class=&quot;section-heading&quot;&gt;A map?&lt;/h2&gt;

Let&#39;s color a map using poverty by county:


{% highlight r %}
pov &lt;- sumacs(formula = &quot;b17001_002 / b17001_001 * 100&quot;,
        varname = c(&quot;pov&quot;),
        method = c(&quot;prop&quot;),
        level = c(&quot;county&quot;),
        state = &quot;*&quot;)
{% endhighlight %}



{% highlight text %}
## [1] &quot;. . . . . .  ACS variables : 2&quot;
## [1] &quot;. . . . . .  Levels : 1&quot;
## [1] &quot;. . . . . .  New variables : 1&quot;
## [1] &quot;. . . . . .  Getting county data&quot;
## [1] &quot;. . . . . .  Done!&quot;
## [1] &quot;. . . . . .  Creating variables&quot;
## [1] &quot;. . . . . .  100%&quot;
## [1] &quot;. . . . . .  Formatting output&quot;
## [1] &quot;. . . . . .  Done!&quot;
{% endhighlight %}


{% highlight r %}
library(choroplethr)
library(choroplethrMaps)
pov[, region := as.numeric(geoid)]
setnames(pov, &quot;pov_est&quot;, &quot;value&quot;)
county_choropleth(pov, num_colors = 5)
{% endhighlight %}

![center](/img/2016-07-06-acsr/unnamed-chunk-14-1.png)

In sum, the `acsr` package:

- Reads formulas directly and extracts any ACS variable
- Provides an automatized and tailored way to obtain indicators and MOEs
- Allows different outputs&#39; formats (wide and long, csv)
- Provides an easy way to adjust MOEs to different confidence levels
- Includes a variable-by-variable ratio adjustment of standard errors
- Includes the zero-option when computing standard errors for proportions, ratios, and aggregations
- Combines geographic levels flexibly



**Last Update: 08/30/2016**</content><category term="R" /><category term="ACS" /><category term="R" /><summary>The acsr package is to extract variables and compute statistics using the America Community Survey in a systematic way. It was created for the Applied Population Laboratory (APT) at the University of Wisconsin-Madison.</summary></entry><entry><title>Imputing scales using parcels of items as auxiliary variables</title><link href="http://sdaza.com/imputation/2015/10/14/imputation_parcels/" rel="alternate" type="text/html" title="Imputing scales using parcels of items as auxiliary variables" /><published>2015-10-14T00:00:00-05:00</published><updated>2015-10-14T00:00:00-05:00</updated><id>http://sdaza.com/imputation/2015/10/14/imputation_parcels</id><content type="html" xml:base="http://sdaza.com/imputation/2015/10/14/imputation_parcels/">**Last Update: 02/07/2016**


Multiple imputation. Ugh. Multiple imputation of scales using several items. Ugh squared! Fortunately, to impute every single item is not the only solution. There are some practical and *theoretically* attractive alternatives! In this post, I show a simple implementation of what Enders (2010) calls **duplicated-scale imputation**. The specific method I show here was proposed by Eekhout et al. (2011). Thanks [Iris Eekhout](http://www.iriseekhout.com) for replying my emails and answering my questions!

## Procedure

For each scale, I define a number (or proportion) of items (let&#39;s say **p**) to create parcels (i.e., average of items). These parcels are used as auxiliary variables to impute the original scales. There are different ways to define parcels. I implemented a solution: see the function *rowscore* available in my R package [sdazar](http://github.com/sdaza/sdazar).

The function `rowscore` select **p** items with less missing data. For each case (row), it computes the parcels using the available information of the selected items. If only one item has information, only that one will be used. If there are more than one item with valid data, it will average the available items. If there are no items available, it will pick **p** items from the rest of items to impute the original scale. In this particular example I created parcels using half of the items:


{% highlight r %}
rowscore(data, items, p = 1/2, type = &quot;parcel&quot;)
{% endhighlight %}
The idea of using a proportion of the original items is to include as much as information possible but preventing strong linear dependencies between the variables. Ideally, after this procedure, *parcels* should be complete. However, because in some cases all the items are missing, parcels can still have missing records (although less than the original scales).

**Why not just to use the average of the available items?** That solution would implicitly assume that items perfectly correlates with the scale. We know that is not a good assumption. That is why, after all, we worry about creating scales. Using parcels takes advantage of the available information (items with complete information) and the relationship between a portion of items and the scale.

Here I show a simple example using the [National Longitudinal Study of Adolescent to Adult Health (Add Health)](http://www.cpc.unc.edu/projects/addhealth). First, let&#39;s look some descriptives of the variables included in the imputation. I am using information from Wave 1 and 2. The key scales/scores are depression (19 items) and GPA (4 items). Variables ending with  `.p` are parcels with 1/2 of the items of the original scale.








{% highlight r %}
dim(dats)
{% endhighlight %}



{% highlight text %}
## [1] 12976    15
{% endhighlight %}



{% highlight r %}
str(dats[, nvars, with = FALSE])
{% endhighlight %}



{% highlight text %}
## Classes &#39;data.table&#39; and &#39;data.frame&#39;:	12976 obs. of  15 variables:
##  $ female   : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 2 1 1 1 1 1 1 ...
##  $ age      : int  16 16 14 13 14 17 14 17 17 14 ...
##  $ race     : Factor w/ 5 levels &quot;white&quot;,&quot;black&quot;,..: 2 1 1 1 2 1 3 3 3 2 ...
##  $ class    : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 2 2 4 1 4 3 4 2 3 ...
##  $ publicass: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 NA ...
##  $ bmi      : num  27.4 16.3 22.2 18.2 21.9 ...
##  $ gpa1     : num  2 3.25 3.75 3.25 2.25 1 NA 2.25 NA 2.5 ...
##  $ gpa2     : num  2.5 2.5 4 3.75 1.75 1.5 NA 3 1 4 ...
##  $ gpa1.p   : num  2.5 3.5 3.5 3 2.5 1 3.5 3 1 3.5 ...
##  $ gpa2.p   : num  2.5 2.5 4 3.5 1.5 2 1.5 3 1 4 ...
##  $ dep1.p   : num  0.2 0.7 0 0.3 0.4 0.4 0.1 1 0.6 1.2 ...
##  $ dep1     : num  0.263 0.789 0 0.211 0.474 ...
##  $ dep2.p   : num  0.6 0.6 0.1 0.3 0.4 0.6 0.2 1.1 0.8 0.5 ...
##  $ dep2     : num  0.6316 0.5263 0.0526 0.1579 0.3684 ...
##  $ ppvt     : int  101 75 121 96 79 97 103 89 82 120 ...
##  - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt;
{% endhighlight %}


{% highlight r %}
# create summary table using package tables
missing &lt;- function (x) {sum(is.na(x))}
tabular(
  ( dep2 + dep2.p + dep1 + dep1.p + gpa2 + gpa2.p +
    gpa1 + gpa1.p  + age + bmi + ppvt )
   ~  (Format(digit = 2) * ( Heading(&quot;Mean&quot;)
   	* sdazar::Mean + Heading(&quot;SD&quot;)
   	* sdazar::Sd + Heading(&quot;Min&quot;)
   	* sdazar::Min + Heading(&quot;Max&quot;)
   	* sdazar::Max + (Heading(&quot;Missing&quot;) * missing ))),
  data = dats)
{% endhighlight %}



{% highlight text %}
##                                                
##         Mean    SD      Min     Max     Missing
##  dep2      0.58    0.39    0.00    2.95   68.00
##  dep2.p    0.58    0.43    0.00    3.00    6.00
##  dep1      0.58    0.39    0.00    2.84   81.00
##  dep1.p    0.62    0.44    0.00    3.00   18.00
##  gpa2      2.86    0.74    1.00    4.00 4710.00
##  gpa2.p    2.76    0.84    1.00    4.00 1238.00
##  gpa1      2.82    0.76    1.00    4.00 2788.00
##  gpa1.p    2.74    0.85    1.00    4.00  342.00
##  age      15.28    1.61   11.00   21.00    9.00
##  bmi      22.37    4.42   11.47   63.47  353.00
##  ppvt    100.17   15.00   13.00  146.00  568.00
{% endhighlight %}

As expected, the correlation between the scales and parcels is high. GPA variables have most of the problems. Note that parcels `.p` still have missing records, although much less than the original scales.


{% highlight r %}
cor(dats[, .(dep1, dep1.p)], use = &quot;complete&quot;)
{% endhighlight %}



{% highlight text %}
##         dep1 dep1.p
## dep1   1.000  0.947
## dep1.p 0.947  1.000
{% endhighlight %}



{% highlight r %}
cor(dats[, .(dep2, dep2.p)], use = &quot;complete&quot;)
{% endhighlight %}



{% highlight text %}
##         dep2 dep2.p
## dep2   1.000  0.948
## dep2.p 0.948  1.000
{% endhighlight %}



{% highlight r %}
cor(dats[, .(gpa1, gpa1.p)], use = &quot;complete&quot;)
{% endhighlight %}



{% highlight text %}
##         gpa1 gpa1.p
## gpa1   1.000  0.888
## gpa1.p 0.888  1.000
{% endhighlight %}



{% highlight r %}
cor(dats[, .(gpa2, gpa2.p)], use = &quot;complete&quot;)
{% endhighlight %}



{% highlight text %}
##         gpa2 gpa2.p
## gpa2   1.000  0.885
## gpa2.p 0.885  1.000
{% endhighlight %}

I use the R package *MICE* to impute the data.


{% highlight r %}
ini &lt;- mice(dats[, nvars, with = FALSE], m = 1, maxit = 0)

# get methods
(meth &lt;- ini$meth)
{% endhighlight %}



{% highlight text %}
##    female       age      race     class publicass       bmi      gpa1      gpa2    gpa1.p    gpa2.p    dep1.p      dep1    dep2.p      dep2      ppvt 
##        &quot;&quot;     &quot;pmm&quot;        &quot;&quot; &quot;polyreg&quot;  &quot;logreg&quot;     &quot;pmm&quot;     &quot;pmm&quot;     &quot;pmm&quot;     &quot;pmm&quot;     &quot;pmm&quot;     &quot;pmm&quot;     &quot;pmm&quot;     &quot;pmm&quot;     &quot;pmm&quot;     &quot;pmm&quot;
{% endhighlight %}



{% highlight r %}
# get predictor matrix
pred &lt;- ini$pred
{% endhighlight %}

I adjusted the predictor matrix to avoid feedbacks during the imputation (circularity between variables). The main adjustment is to use only complete variables when imputing *parcels*.


{% highlight r %}
# predict parcels only with complete variables to avoid feedbacks
pred[&quot;gpa1.p&quot;, ] &lt;- 0
pred[&quot;gpa2.p&quot;, ] &lt;- 0
pred[&quot;dep1.p&quot;, ] &lt;- 0
pred[&quot;dep2.p&quot;, ] &lt;- 0

pred[&quot;gpa1.p&quot;, c(&quot;female&quot;, &quot;race&quot;)] &lt;- 1
pred[&quot;gpa2.p&quot;, c(&quot;female&quot;, &quot;race&quot;)] &lt;- 1
pred[&quot;dep1.p&quot;, c(&quot;female&quot;, &quot;race&quot;)] &lt;- 1
pred[&quot;dep2.p&quot;, c(&quot;female&quot;, &quot;race&quot;)] &lt;- 1

# predict scales using parcels
pred[, &quot;gpa1.p&quot;] &lt;- 0
pred[, &quot;gpa2.p&quot;] &lt;- 0
pred[, &quot;dep1.p&quot;] &lt;- 0
pred[, &quot;dep2.p&quot;] &lt;- 0

pred[&quot;gpa1&quot;, c(&quot;gpa1.p&quot;)] &lt;- 1
pred[&quot;gpa2&quot;, c(&quot;gpa2.p&quot;)] &lt;- 1
pred[&quot;dep1&quot;, c(&quot;dep1.p&quot;)] &lt;- 1
pred[&quot;dep2&quot;, c(&quot;dep2.p&quot;)] &lt;- 1
{% endhighlight %}

Here the adjusted predictor matrix:


{% highlight text %}
##           female age race class publicass bmi gpa1 gpa2 gpa1.p gpa2.p dep1.p dep1 dep2.p dep2 ppvt
## female         0   0    0     0         0   0    0    0      0      0      0    0      0    0    0
## age            1   0    1     1         1   1    1    1      0      0      0    1      0    1    1
## race           0   0    0     0         0   0    0    0      0      0      0    0      0    0    0
## class          1   1    1     0         1   1    1    1      0      0      0    1      0    1    1
## publicass      1   1    1     1         0   1    1    1      0      0      0    1      0    1    1
## bmi            1   1    1     1         1   0    1    1      0      0      0    1      0    1    1
## gpa1           1   1    1     1         1   1    0    1      1      0      0    1      0    1    1
## gpa2           1   1    1     1         1   1    1    0      0      1      0    1      0    1    1
## gpa1.p         1   0    1     0         0   0    0    0      0      0      0    0      0    0    0
## gpa2.p         1   0    1     0         0   0    0    0      0      0      0    0      0    0    0
## dep1.p         1   0    1     0         0   0    0    0      0      0      0    0      0    0    0
## dep1           1   1    1     1         1   1    1    1      0      0      1    0      0    1    1
## dep2.p         1   0    1     0         0   0    0    0      0      0      0    0      0    0    0
## dep2           1   1    1     1         1   1    1    1      0      0      0    1      1    0    1
## ppvt           1   1    1     1         1   1    1    1      0      0      0    1      0    1    0
{% endhighlight %}

Let&#39;s impute the data!


{% highlight r %}
imp &lt;- mice(dats[, nvars, with = FALSE],
	pred = pred, m = 5, maxit = 10)
{% endhighlight %}

Some plots to explore how the imputation went.


{% highlight r %}
plot(imp, c(&quot;gpa1&quot;, &quot;gpa2&quot;, &quot;dep1&quot;, &quot;dep2&quot;))
{% endhighlight %}

![center](/img/2015-10-14-imputation_parcels/unnamed-chunk-12-1.png)![center](/img/2015-10-14-imputation_parcels/unnamed-chunk-12-2.png)

I don&#39;t see any problematic pattern. It looks as I got a proper solution. The distribution of the variables also looks right.


{% highlight r %}
densityplot(imp, ~ gpa1 + gpa2 + dep1 + dep2)
{% endhighlight %}

![center](/img/2015-10-14-imputation_parcels/unnamed-chunk-13-1.png)

{% highlight r %}
bwplot(imp, gpa1 + gpa2 + dep1 + dep2  ~ .imp)
{% endhighlight %}

![center](/img/2015-10-14-imputation_parcels/unnamed-chunk-13-2.png)


-----

### References

Enders, Craig K. 2010. *Applied Missing Data Analysis*. The Guilford Press.

Eekhout, Iris, Craig K. Enders, Jos W. R. Twisk, Michiel R. de Boer, Henrica
C. W. de Vet, and Martijn W. Heymans. 2015. &quot;Analyzing Incomplete Item Scores
in Longitudinal Data by Including Item Score Information as Auxiliary
Variables.&quot; *Structural Equation Modeling: A Multidisciplinary Journal* 22
(4):588-602.</content><category term="imputation" /><summary>Last Update: 02/07/2016</summary></entry><entry><title>Simple R package to define sample sizes and MOEs</title><link href="http://sdaza.com/survey/2015/09/30/sampler/" rel="alternate" type="text/html" title="Simple R package to define sample sizes and MOEs" /><published>2015-09-30T00:00:00-05:00</published><updated>2015-09-30T00:00:00-05:00</updated><id>http://sdaza.com/survey/2015/09/30/sampler</id><content type="html" xml:base="http://sdaza.com/survey/2015/09/30/sampler/">In this post I present a simple R package called [**sampler**](https://github.com/sdaza/sampler). The package defines **sample sizes** and **margins of error (MOE)** for a proportion, as usually is done when designing public opinion surveys. [In a previous post](/survey/2014/01/19/samplesize/), I presented some functions that do mostly the same thing. This new package, however, includes some new features that might be useful. 

## Installation


{% highlight r %}
# you have to install devtools first
devtools::install_github(&quot;sdaza/sampler&quot;) 
library(sampler)
{% endhighlight %}

## Functions

The packages contains four functions:

- **ssize**: computes sample size.
- **serr**: computes MOE.
- **astrata**: assigns sample sizes to strata.
- **serrst**: computes MOE for stratified samples.





## Define sample size: *ssize* 


{% highlight r %}
ssize(.05)
{% endhighlight %}



{% highlight text %}
## [1] 384
{% endhighlight %}



{% highlight r %}
# design effect (deff) and response rate (rr)
ssize(.05, deff = 1.2, rr = .90) 
{% endhighlight %}



{% highlight text %}
## [1] 512
{% endhighlight %}



{% highlight r %}
# finite population correction
ssize(.05, deff = 1.2, rr = .90, N = 1000) 
{% endhighlight %}



{% highlight text %}
## [1] 370
{% endhighlight %}



{% highlight r %}
# warning message
ssize(.05, deff = 1.2, rr = .90, N = 100) 
{% endhighlight %}



{% highlight text %}
## n is bigger than N in some rows: n = N
{% endhighlight %}



{% highlight text %}
## [1] 100
{% endhighlight %}

## Define sampling error: *serr*


{% highlight r %}
serr(384)
{% endhighlight %}



{% highlight text %}
## [1] 0.05
{% endhighlight %}



{% highlight r %}
serr(512, deff = 1.2, rr = .90)
{% endhighlight %}



{% highlight text %}
## [1] 0.05
{% endhighlight %}



{% highlight r %}
serr(370, deff = 1.2, rr = .90, N = 1000)
{% endhighlight %}



{% highlight text %}
## [1] 0.05
{% endhighlight %}



{% highlight r %}
# we still get an answer
serr(100, deff = 1.2, rr = .90, N = 100) 
{% endhighlight %}



{% highlight text %}
## [1] 0.0569
{% endhighlight %}

## Strata allocation: *astrata*

These examples show how to allocate a sample size across strata. Look at *?astrata* in **R** for definitions of the allocation procedures that are available. 


{% highlight r %}
# I will use data.table
library(data.table) 
chile &lt;- data.table(chile)
chile
{% endhighlight %}



{% highlight text %}
##     reg     pob  pr
##  1:   1  328782 0.3
##  2:   2  613328 0.4
##  3:   3  308247 0.5
##  4:   4  759228 0.5
##  5:   5 1808300 0.5
##  6:   6  910577 0.6
##  7:   7 1035593 0.3
##  8:   8 2100494 0.1
##  9:   9  983499 0.2
## 10:  10  834714 0.5
## 11:  11  107334 0.5
## 12:  12  163748 0.4
## 13:  13 7228581 0.6
## 14:  14  401548 0.2
## 15:  15  235081 0.3
{% endhighlight %}


{% highlight r %}
# proportional for a sample of 1000
chile[, aprop := astrata(1000, wp = 1, N = pob)] 

# fixed (same number by stratum)
chile[, afixed := astrata(1000, wp = 0, N = pob)] 

# 40% proportional, 60% fixed
chile[, a40 := astrata(1000, wp =.4, N = pob)] 

# 60% proportional, 40% fixed
chile[, a60 := astrata(1000, wp =.6, N = pob)] 

# square-root
chile[, aroot := astrata(1000, method = &quot;root&quot;, N = pob)]

# neyman
chile[, aneyman := astrata(1000, method = &quot;neyman&quot;, N = pob, p = pr)] 

# standard deviation
chile[, astdev := astrata(1000, method = &quot;stdev&quot;, N = pob, p = pr)] 

# error
chile[, aerr := astrata(e = .11, method = &quot;error&quot;, N = pob, p = pr)] 
{% endhighlight %}


{% highlight text %}
##     reg     pob  pr aprop afixed a40 a60 aroot aneyman astdev aerr
##  1:   1  328782 0.3    18     67  47  38    41      18     66   67
##  2:   2  613328 0.4    34     67  54  47    56      37     71   76
##  3:   3  308247 0.5    17     67  47  37    40      19     72   79
##  4:   4  759228 0.5    43     67  57  53    62      46     72   79
##  5:   5 1808300 0.5   101     67  81  87    96     110     72   79
##  6:   6  910577 0.6    51     67  61  57    68      54     71   76
##  7:   7 1035593 0.3    58     67  63  62    73      58     66   67
##  8:   8 2100494 0.1   118     67  87  98   104      77     43   29
##  9:   9  983499 0.2    55     67  62  60    71      48     58   51
## 10:  10  834714 0.5    47     67  59  55    65      51     72   79
## 11:  11  107334 0.5     6     67  43  30    23       7     72   79
## 12:  12  163748 0.4     9     67  44  32    29      10     71   76
## 13:  13 7228581 0.6   406     67 203 270   192     432     71   76
## 14:  14  401548 0.2    23     67  49  41    45      20     58   51
## 15:  15  235081 0.3    13     67  45  35    35      13     66   67
{% endhighlight %}

## Getting sampling error from a stratified sample: *serrst*


{% highlight r %}
# the second most efficient allocation
serrst(n = chile$aprop, N = chile$pob, p = chile$pr)
{% endhighlight %}



{% highlight text %}
## [1] 0.0288
{% endhighlight %}



{% highlight r %}
# the worst solution
serrst(n = chile$afixed, N = chile$pob, p = chile$pr)
{% endhighlight %}



{% highlight text %}
## [1] 0.0518
{% endhighlight %}



{% highlight r %}
serrst(n = chile$a40, N = chile$pob, p = chile$pr)
{% endhighlight %}



{% highlight text %}
## [1] 0.0339
{% endhighlight %}



{% highlight r %}
serrst(n = chile$a60, N = chile$pob, p = chile$pr)
{% endhighlight %}



{% highlight text %}
## [1] 0.0311
{% endhighlight %}



{% highlight r %}
serrst(n = chile$aroot, N = chile$pob, p = chile$pr)
{% endhighlight %}



{% highlight text %}
## [1] 0.0339
{% endhighlight %}



{% highlight r %}
# the most efficient allocation
serrst(n = chile$aneyman, N = chile$pob, p = chile$pr) 
{% endhighlight %}



{% highlight text %}
## [1] 0.0285
{% endhighlight %}



{% highlight r %}
serrst(n = chile$astdev, N = chile$pob, p = chile$pr)
{% endhighlight %}



{% highlight text %}
## [1] 0.0508
{% endhighlight %}



{% highlight r %}
serrst(n = chile$aerr, N = chile$pob, p = chile$pr)
{% endhighlight %}



{% highlight text %}
## [1] 0.0498
{% endhighlight %}

## Combining criteria


{% highlight r %}
# get error for 60% proportional / 40% fixed allocation for each strata
chile[, error_a60 := serr(a60, p = pr)] 

# assign sample sizes assuming 13% error for each strata
chile[, serr13 := astrata(e = .13, method = &quot;error&quot;, N = pob, p = pr)] 

# total error, not that good!
serrst(n = chile$serr13, N = chile$pob, p = chile$pr) 
{% endhighlight %}



{% highlight text %}
## [1] 0.0586
{% endhighlight %}



{% highlight r %}
chile[, .(reg, pob, pr, a60, error_a60, serr13)]
{% endhighlight %}



{% highlight text %}
##     reg     pob  pr a60 error_a60 serr13
##  1:   1  328782 0.3  38    0.1457     48
##  2:   2  613328 0.4  47    0.1401     55
##  3:   3  308247 0.5  37    0.1611     57
##  4:   4  759228 0.5  53    0.1346     57
##  5:   5 1808300 0.5  87    0.1051     57
##  6:   6  910577 0.6  57    0.1272     55
##  7:   7 1035593 0.3  62    0.1141     48
##  8:   8 2100494 0.1  98    0.0594     20
##  9:   9  983499 0.2  60    0.1012     36
## 10:  10  834714 0.5  55    0.1321     57
## 11:  11  107334 0.5  30    0.1789     57
## 12:  12  163748 0.4  32    0.1697     55
## 13:  13 7228581 0.6 270    0.0584     55
## 14:  14  401548 0.2  41    0.1224     36
## 15:  15  235081 0.3  35    0.1518     48
{% endhighlight %}

We can adjust a bit more:


{% highlight r %}
# when error is higher than .13, use serr13
chile[, sfinal := ifelse(error_a60 &gt; .13, serr13, a60)] 

# new error by stratum
chile[, error_sfinal := serr(sfinal, p = pr)]

# total error, much better!
serrst(n = chile$sfinal, N = chile$pob, p = chile$pr)
{% endhighlight %}



{% highlight text %}
## [1] 0.0309
{% endhighlight %}



{% highlight r %}
# although the total sample size is now bigger
sum(chile$sfinal)
{% endhighlight %}



{% highlight text %}
## [1] 1109
{% endhighlight %}

{% highlight text %}
##     reg     pob  pr sfinal error_sfinal
##  1:   1  328782 0.3     48       0.1296
##  2:   2  613328 0.4     55       0.1295
##  3:   3  308247 0.5     57       0.1298
##  4:   4  759228 0.5     57       0.1298
##  5:   5 1808300 0.5     87       0.1051
##  6:   6  910577 0.6     57       0.1272
##  7:   7 1035593 0.3     62       0.1141
##  8:   8 2100494 0.1     98       0.0594
##  9:   9  983499 0.2     60       0.1012
## 10:  10  834714 0.5     57       0.1298
## 11:  11  107334 0.5     57       0.1298
## 12:  12  163748 0.4     55       0.1295
## 13:  13 7228581 0.6    270       0.0584
## 14:  14  401548 0.2     41       0.1224
## 15:  15  235081 0.3     48       0.1296
{% endhighlight %}

That&#39;s it. A simple package to do simple calculations.</content><category term="survey" /><summary>In this post I present a simple R package called sampler. The package defines sample sizes and margins of error (MOE) for a proportion, as usually is done when designing public opinion surveys. In a previous post, I presented some functions that do mostly the same thing. This new package, however, includes some new features that might be useful.</summary></entry><entry><title>Functions for sample size and error</title><link href="http://sdaza.com/survey/2014/01/19/samplesize/" rel="alternate" type="text/html" title="Functions for sample size and error" /><published>2014-01-19T00:00:00-06:00</published><updated>2014-01-19T00:00:00-06:00</updated><id>http://sdaza.com/survey/2014/01/19/samplesize</id><content type="html" xml:base="http://sdaza.com/survey/2014/01/19/samplesize/">Here I show two functions in R to define sample sizes and errors of a proportion, taking into account design effect, response rate, finite population correction, and stratification. They are useful when one needs to do these calculations quickly.

**Note: I created a package with similar functions. [See here](/survey/2015/09/30/sampler/).**

The inputs are: 

- **n** = sample size
- **e** = sampling error
- **deff** = design effect, by default 1 (SRS)
- **rr** = response rate, by default 1
- **N** = population size, by default NULL (infinite population)
- **cl** = confidence level , by default .95  
- **p** = proportion, by default 0.5 (maximum variance of a proportion)
- **relative** = to estimate relative error, by default FALSE

## first, load the functions




{% highlight r %}
library(devtools); source_gist(&quot;7896840&quot;)
{% endhighlight %}
 
## serr: sampling error

An example for n = 400 and all inputs at their default values: 


{% highlight r %}
serr(400)
{% endhighlight %}



{% highlight text %}
## [1] 0.049
{% endhighlight %}

The output is rounded to 4 decimals. A more complete example: 

- **n** = 400
- **deff** = 1.5
- **response rate** = 80%
- **population size** = 1000 


{% highlight r %}
serr(n=400, deff=1.5, rr=.8, N=1000)
{% endhighlight %}



{% highlight text %}
## [1] 0.0595
{% endhighlight %}

The sample size (n) has always to be lower than the population (N).  It is important to note that the final sample size used to compute the sampling error is: 

$$ n = \frac{N}{deff} * rr$$


{% highlight r %}
serr(n=400, N=350)
{% endhighlight %}



{% highlight text %}
## Error: n is bigger than N
{% endhighlight %}

## ssize: sample size
	
Let&#39;s get a sample size with an error of .03, a population of 1000 elements, a response rate of 0.80, and an effect design of 1.2: 


{% highlight r %}
ssize(e=.03, deff=1.2, rr=.8, N=1000)
{% endhighlight %}



{% highlight text %}
## [1] 775
{% endhighlight %}

If the the sample size is bigger than the population because of low response rates or big design effects, the sample size will be fixed to N: 


{% highlight r %}
ssize(e=.03, deff=5, rr=.6, N=1000)
{% endhighlight %}



{% highlight text %}
## n is bigger than N in some rows: n = N
{% endhighlight %}



{% highlight text %}
## [1] 1000
{% endhighlight %}

## Working with strata

Finally, we can estimate different sample sizes by strata using vectors or a data frame: 


{% highlight r %}
# example sampling frame (4 strata)
frame &lt;- data.frame(
	strata = 1:4, 
	N =c(10000, 5000, 2000, 1000), 
	deff =c(1.1, 1, 1.3, .8), 
	rr = c(.8, .9, .85,.8),
	p = c(.3, .6, .1, .2))
{% endhighlight %}


{% highlight text %}
##   strata     N deff   rr   p
## 1      1 10000  1.1 0.80 0.3
## 2      2  5000  1.0 0.90 0.6
## 3      3  2000  1.3 0.85 0.1
## 4      4  1000  0.8 0.80 0.2
{% endhighlight %}


{% highlight r %}
frame$n1 &lt;- ssize(e=.02, deff=frame$deff, rr=frame$rr, N=frame$N, p=frame$p)
frame$e1 &lt;- serr(n=frame$n1, deff=frame$deff, rr=frame$rr, N=frame$N, p=frame$p)
{% endhighlight %}


{% highlight text %}
##   strata     N deff   rr   p   n1   e1
## 1      1 10000  1.1 0.80 0.3 2308 0.02
## 2      2  5000  1.0 0.90 0.6 1753 0.02
## 3      3  2000  1.3 0.85 0.1  923 0.02
## 4      4  1000  0.8 0.80 0.2  606 0.02
{% endhighlight %}

As easy as falling off a log!</content><category term="survey" /><summary>Here I show two functions in R to define sample sizes and errors of a proportion, taking into account design effect, response rate, finite population correction, and stratification. They are useful when one needs to do these calculations quickly.</summary></entry><entry><title>Migration: forward survival method</title><link href="http://sdaza.com/demography/2013/07/03/projecMigration/" rel="alternate" type="text/html" title="Migration: forward survival method" /><published>2013-07-03T00:00:00-05:00</published><updated>2013-07-03T00:00:00-05:00</updated><id>http://sdaza.com/demography/2013/07/03/projecMigration</id><content type="html" xml:base="http://sdaza.com/demography/2013/07/03/projecMigration/">This is a question about estimating incoming and outgoing movements of population. I have data on enrollment at a small college in 2009 and 2010, and also the information on student-years enrolled before students drop out of the college ($_nL_x$). Each year 400 freshman enroll. What is the number of net transfers (incoming minus outgoing) between 2009 and 2010?

|     | 2009  | 2010 | $_nL_x$ |
|:--- | :--- | :--- | :--- |
| Freshman | 400 | 400  | 99 |
| Sophomore | 350 | 389  | 90 |
| Junior | 300 | 420 | 80 |
| Senior | 300 | 350  | 78 |
| Higher | 300 | 300  | 77 |

Basically, I have to estimate the &quot;extra&quot;  population observed during 2010 that doesn&#39;t come from enrollment (births) or mortality (drop-outs). That is, to project the 2009 population assuming that is closed to migration. Just looking at the table one can see that the population is growing due to transfers (mortality and births are constant).

First, I estimate the survival ratios: 





{% highlight r %}
p2009 &lt;- c(400, 350, 300, 300, 300)
p2010 &lt;- c(400, 389, 420, 350, 300)
Lx &lt;- c(99, 90, 80, 78, 77)
{% endhighlight %}



{% highlight r %}
# survival ratios
sr &lt;- NA
for (i in 1:(length(Lx) - 1)) {
    sr[i] &lt;- Lx[i + 1]/Lx[i]
}

# open-end interval
sr[length(sr)] &lt;- Lx[5]/(Lx[4] + Lx[5])
{% endhighlight %}



{% highlight text %}
## [1] 0.909 0.889 0.975 0.497
{% endhighlight %}


Second, I define the matrix to do the projection: 


{% highlight r %}
m &lt;- matrix(0, 5, 5)

# The population of the first interval has always to be
# 400*99/100 (births are constant)
m[1, 1] &lt;- 99/100
# Because the population of the first interval is 400 in
# 2009 I use this shortcut
s &lt;- diag(4)
s &lt;- s * sr
m[2:5, 1:4] &lt;- s
m[5, 5] &lt;- sr[4]
{% endhighlight %}


The result is not exactly a Leslie matrix because I don&#39;t have fertility rates in the first row.


{% highlight text %}
##       [,1]  [,2]  [,3]  [,4]  [,5]
## [1,] 0.990 0.000 0.000 0.000 0.000
## [2,] 0.909 0.000 0.000 0.000 0.000
## [3,] 0.000 0.889 0.000 0.000 0.000
## [4,] 0.000 0.000 0.975 0.000 0.000
## [5,] 0.000 0.000 0.000 0.497 0.497
{% endhighlight %}


Finally, I can project forward: 


{% highlight r %}
proj2010 &lt;- m %*% p2009
{% endhighlight %}



{% highlight text %}
##      [,1]
## [1,]  396
## [2,]  364
## [3,]  311
## [4,]  292
## [5,]  298
{% endhighlight %}


The number of net transfers would be the differences between the observed population in 2010, and the 2010 projected population assuming that it is closed to migration (no transfers). 


{% highlight r %}
sum(p2010 - proj2010)
{% endhighlight %}



{% highlight text %}
## [1] 198
{% endhighlight %}


This was an example of the forward survival method to estimate migration.</content><category term="demography" /><summary>This is a question about estimating incoming and outgoing movements of population. I have data on enrollment at a small college in 2009 and 2010, and also the information on student-years enrolled before students drop out of the college ($_nL_x$). Each year 400 freshman enroll. What is the number of net transfers (incoming minus outgoing) between 2009 and 2010?</summary></entry><entry><title>Cohort component projection</title><link href="http://sdaza.com/demography/2013/07/02/projections/" rel="alternate" type="text/html" title="Cohort component projection" /><published>2013-07-02T00:00:00-05:00</published><updated>2013-07-02T00:00:00-05:00</updated><id>http://sdaza.com/demography/2013/07/02/projections</id><content type="html" xml:base="http://sdaza.com/demography/2013/07/02/projections/">I present an example of a cohort component projection using a closed female population (Sweden 1993), taken from Preston et al.&#39;s book (Demography 2001, page 125). I use R and basic matrix algebra to replicate their results. The advantage of this procedure is that allows to compute easily the *intrinsic growth rate* and *age-proportionate distribution* of the stable equivalent population. All we need is the population by age at time 0 (from a census), survivorship ratios (from a life table), and age-specific fertility rates. 

The data: 





{% highlight r %}
dat &lt;- read.csv(&quot;sweden1993.csv&quot;, sep = &quot;,&quot;, header = T)
attach(dat)
{% endhighlight %}



{% highlight text %}
##    age     Nf     Lf      f
## 1    0 293395 497487     NA
## 2    5 248369 497138     NA
## 3   10 240012 496901     NA
## 4   15 261346 496531 0.0120
## 5   20 285209 495902 0.0908
## 6   25 314388 495168 0.1499
## 7   30 281290 494213 0.1125
## 8   35 286923 492760 0.0441
## 9   40 304108 490447 0.0074
## 10  45 324946 486613 0.0003
## 11  50 247613 480665     NA
## 12  55 211351 471786     NA
## 13  60 215140 457852     NA
## 14  65 221764 436153     NA
## 15  70 223506 402775     NA
## 16  75 183654 350358     NA
## 17  80 141990 271512     NA
## 18  85 112424 291707     NA
{% endhighlight %}


As can be seen, the data have five-year-interval age groups, so each projection forward will involve 5 years. The steps are very simple: 

1. Project forward the population of each age group (estimation of people alive)
2. Calculate the number of births of each age group based on fertility rates, adjusting by mortality (estimation of children alive)
3. Create a Leslie matrix, and then multiple it by the population vector (population by age at time 0) 

### Survivorship ratios

We have to estimate life table survival ratios, that is, proportions of birth cohorts surviving from one age interval to the next in a **stationary population**. Basically, we are summarizing the mortality experience of different cohorts assuming stationarity. Because census statistics refer to age &quot;last birthday&quot; (rather than exact age), I estimate ratios using $L_x$ (average number of survivors in an age interval) instead of $l_x$. 

$$S_x = \frac{_5L_x}{_5L_{x-5}}$$

I compute the survival ratios using a loop in R. The estimation of the open-ended survival ratio is slightly different but still straightforward: 

$$\frac{T_{85}}{T_{80}}$$  


{% highlight r %}
Sf &lt;- NA
for (i in 1:(length(Lf) - 1)) {
    Sf[i] &lt;- Lf[i + 1]/Lf[i]
}

# open-ended survival ratio
Sf[length(Sf)] &lt;- Lf[18]/(Lf[17] + Lf[18])
{% endhighlight %}



{% highlight text %}
##  [1] 0.999 1.000 0.999 0.999 0.999 0.998 0.997 0.995 0.992 0.988 0.982 0.970 0.953 0.923 0.870 0.775 0.518
{% endhighlight %}

### Number of children

This is the tricky part. Because census statistics refer to age &quot;last birthday&quot;, and we are projecting every 5 years, the estimation of the number of person-years lived by women in each age group consists of the average number of women alive at the beginning and end of the period (assuming a linear change over the period). To take advantage of the Leslie matrix, I define the births in R using a loop as follows: 


{% highlight r %}
Bf &lt;- rep(0, 18)
for (i in 1:length(Lf)) {
    Bf[i] &lt;- 1/(1 + 1.05) * Lf[1]/(100000 * 2) * sum(f[i + 1] * Sf[i], f[i], na.rm = TRUE)
}
{% endhighlight %}



{% highlight text %}
##  [1] 0.000000 0.000000 0.014550 0.124596 0.291792 0.318128 0.189858 0.062447 0.009340 0.000364 0.000000 0.000000
## [13] 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000
{% endhighlight %}


1/(1+1.05) corresponds to a transformation of age-specific fertility rates (son and daughters) to maternity rates (only daughters), assuming that the ratio of male to female births (SBR) is constant across mothers&#39; ages. The number of births is also adjusted by the corresponding survival ratio from 0 to 5 years old ($\frac{_5L_0}{5 \times l_0}$), the number 5 goes away due to simplifying).

### Leslie matrix

I construct a Leslie matrix by replacing specific cells of a 18 x 18 matrix (18 age groups) by the vectors defined above (survival ratios and maternity rates):


{% highlight r %}
m &lt;- matrix(0, 18, 18)
m[1, ] &lt;- Bf
s &lt;- diag(17) * Sf
m[2:18, 1:17] &lt;- s
m[18, 18] &lt;- Sf[17]
{% endhighlight %}


Here we have the Leslie matrix:


{% highlight text %}
##        [,1] [,2]   [,3]  [,4]  [,5]  [,6]  [,7]   [,8]    [,9]    [,10] [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18]
##  [1,] 0.000    0 0.0145 0.125 0.292 0.318 0.190 0.0624 0.00934 0.000364 0.000  0.00 0.000 0.000  0.00 0.000 0.000 0.000
##  [2,] 0.999    0 0.0000 0.000 0.000 0.000 0.000 0.0000 0.00000 0.000000 0.000  0.00 0.000 0.000  0.00 0.000 0.000 0.000
##  [3,] 0.000    1 0.0000 0.000 0.000 0.000 0.000 0.0000 0.00000 0.000000 0.000  0.00 0.000 0.000  0.00 0.000 0.000 0.000
##  [4,] 0.000    0 0.9993 0.000 0.000 0.000 0.000 0.0000 0.00000 0.000000 0.000  0.00 0.000 0.000  0.00 0.000 0.000 0.000
##  [5,] 0.000    0 0.0000 0.999 0.000 0.000 0.000 0.0000 0.00000 0.000000 0.000  0.00 0.000 0.000  0.00 0.000 0.000 0.000
##  [6,] 0.000    0 0.0000 0.000 0.999 0.000 0.000 0.0000 0.00000 0.000000 0.000  0.00 0.000 0.000  0.00 0.000 0.000 0.000
##  [7,] 0.000    0 0.0000 0.000 0.000 0.998 0.000 0.0000 0.00000 0.000000 0.000  0.00 0.000 0.000  0.00 0.000 0.000 0.000
##  [8,] 0.000    0 0.0000 0.000 0.000 0.000 0.997 0.0000 0.00000 0.000000 0.000  0.00 0.000 0.000  0.00 0.000 0.000 0.000
##  [9,] 0.000    0 0.0000 0.000 0.000 0.000 0.000 0.9953 0.00000 0.000000 0.000  0.00 0.000 0.000  0.00 0.000 0.000 0.000
## [10,] 0.000    0 0.0000 0.000 0.000 0.000 0.000 0.0000 0.99218 0.000000 0.000  0.00 0.000 0.000  0.00 0.000 0.000 0.000
## [11,] 0.000    0 0.0000 0.000 0.000 0.000 0.000 0.0000 0.00000 0.987777 0.000  0.00 0.000 0.000  0.00 0.000 0.000 0.000
## [12,] 0.000    0 0.0000 0.000 0.000 0.000 0.000 0.0000 0.00000 0.000000 0.982  0.00 0.000 0.000  0.00 0.000 0.000 0.000
## [13,] 0.000    0 0.0000 0.000 0.000 0.000 0.000 0.0000 0.00000 0.000000 0.000  0.97 0.000 0.000  0.00 0.000 0.000 0.000
## [14,] 0.000    0 0.0000 0.000 0.000 0.000 0.000 0.0000 0.00000 0.000000 0.000  0.00 0.953 0.000  0.00 0.000 0.000 0.000
## [15,] 0.000    0 0.0000 0.000 0.000 0.000 0.000 0.0000 0.00000 0.000000 0.000  0.00 0.000 0.923  0.00 0.000 0.000 0.000
## [16,] 0.000    0 0.0000 0.000 0.000 0.000 0.000 0.0000 0.00000 0.000000 0.000  0.00 0.000 0.000  0.87 0.000 0.000 0.000
## [17,] 0.000    0 0.0000 0.000 0.000 0.000 0.000 0.0000 0.00000 0.000000 0.000  0.00 0.000 0.000  0.00 0.775 0.000 0.000
## [18,] 0.000    0 0.0000 0.000 0.000 0.000 0.000 0.0000 0.00000 0.000000 0.000  0.00 0.000 0.000  0.00 0.000 0.518 0.518
{% endhighlight %}


Note that the last survival ratio is repeated in the last column (0.518). This is because the estimation of the open-ended survival ratio is:

$$ (N_{80} + N_{85}) \times \frac{T_{85}}{T_{80}} $$

### Now, let&#39;s do some projections

Using the R multiplication operator for matrices, I do a 5-year projection  by simply multiplying the Leslie matrix by the population vector (remember that matrix multiplication is not commutative).


{% highlight r %}
m %*% Nf
{% endhighlight %}



{% highlight text %}
##         [,1]
##  [1,] 293574
##  [2,] 293189
##  [3,] 248251
##  [4,] 239833
##  [5,] 261015
##  [6,] 284787
##  [7,] 313782
##  [8,] 280463
##  [9,] 285576
## [10,] 301731
## [11,] 320974
## [12,] 243039
## [13,] 205109
## [14,] 204944
## [15,] 204793
## [16,] 194419
## [17,] 142324
## [18,] 131768
{% endhighlight %}


I obtain the same results of the book. Raising this multiplication I can get the projected population of subsequent periods. Because R doesn&#39;t have a power operator for matrices, I define a function called *mp* to raise matrices (it is not very efficient, but for this example it&#39;s still useful).


{% highlight r %}
mp &lt;- function(mat, pow) {
    ans &lt;- mat
    for (i in 1:(pow - 1)) {
        ans &lt;- mat %*% ans
    }
    return(ans)
}
{% endhighlight %}


Let&#39;s project the initial population for two periods (10 years): 


{% highlight r %}
(mp(m, 2) %*% Nf)
{% endhighlight %}



{% highlight text %}
##         [,1]
##  [1,] 280121
##  [2,] 293368
##  [3,] 293049
##  [4,] 248066
##  [5,] 239529
##  [6,] 260629
##  [7,] 284238
##  [8,] 312859
##  [9,] 279147
## [10,] 283344
## [11,] 298043
## [12,] 315045
## [13,] 235861
## [14,] 195388
## [15,] 189260
## [16,] 178141
## [17,] 150666
## [18,] 141960
{% endhighlight %}


Again, I get the same result of the book. The nice thing of all this is that estimating eigenvalues and eigenvectors, I can obtain the intrinsic growth rate and age-distribution of the &quot;stable equivalent&quot; population. Using the *eigen* function in R, I can identify the dominant eigenvalue (higher absolute number), and the corresponding eigenvector: 


{% highlight r %}
e &lt;- eigen(m)

# intrinsic growth rate
(max(abs(e$values)) - 1)/5  # 5-year-projection
{% endhighlight %}



{% highlight text %}
## [1] 0.000223
{% endhighlight %}



{% highlight r %}

# intrinsic proportionate age distribution
as.numeric(e$vector[, 1]/sum(e$vector[, 1]))
{% endhighlight %}



{% highlight text %}
##  [1] 0.0619 0.0618 0.0617 0.0616 0.0614 0.0613 0.0611 0.0608 0.0605 0.0600 0.0592 0.0580 0.0562 0.0535 0.0494 0.0429
## [17] 0.0332 0.0356
{% endhighlight %}


The population is growing but little. 

### What about the population momentum?

The population momentum corresponds to the growth of a population after imposing replacement fertility conditions, that is, NRR=1. Thus, the first thing we have to do is to estimate NRR. 


{% highlight r %}
# calculating NRR
(NRR &lt;- sum(f * Lf/100000 * (1/(1 + 1.05)), na.rm = TRUE))
{% endhighlight %}



{% highlight text %}
## [1] 1.01
{% endhighlight %}

We can quickly estimate the intrinsic growth rate using NRR: 


{% highlight r %}
# quick estimation of the intrinsic growth rate
log(NRR)/27
{% endhighlight %}



{% highlight text %}
## [1] 0.000237
{% endhighlight %}


Very close to our estimation using cohort component projection. To impose the replacement condition, I just have to divide the first row of the Leslie matrix by NRR.


{% highlight r %}
m[1, ] &lt;- m[1, ]/NRR
{% endhighlight %}


To get the population momentum we have to project the initial population until the growth is zero (here I raised the matrix 100 times), and then to compute the ratio between the initial population and the non-growing population (stationary). 



{% highlight r %}
# population momentum
sum(mp(m, 100) %*% Nf)/sum(Nf)
{% endhighlight %}



{% highlight text %}
## [1] 1.01
{% endhighlight %}


After imposing the replacement condition, the population grew 1%.</content><category term="demography" /><summary>I present an example of a cohort component projection using a closed female population (Sweden 1993), taken from Preston et al.s book (Demography 2001, page 125). I use R and basic matrix algebra to replicate their results. The advantage of this procedure is that allows to compute easily the intrinsic growth rate and age-proportionate distribution of the stable equivalent population. All we need is the population by age at time 0 (from a census), survivorship ratios (from a life table), and age-specific fertility rates.</summary></entry><entry><title>Looking for variables in R</title><link href="http://sdaza.com/r/2013/01/29/lookvar/" rel="alternate" type="text/html" title="Looking for variables in R" /><published>2013-01-29T00:00:00-06:00</published><updated>2013-01-29T00:00:00-06:00</updated><id>http://sdaza.com/r/2013/01/29/lookvar</id><content type="html" xml:base="http://sdaza.com/r/2013/01/29/lookvar/">Recently, I have been working with big databases. After reading their codebooks (usually very long pdf files), 
I thought it would be useful to have a function to find variable names in R. I wrote a simply function 
that looks for variable names in __data.frame__ and __data.table__ objects.

Here an example: 




{% highlight r %}
library(devtools); source_gist(&quot;4661324&quot;)
{% endhighlight %}


{% highlight r %}
library(data.table)
{% endhighlight %}


{% highlight r %}
dat  &lt;- data.table(infert)
(var  &lt;- lookvar(dat, c(&quot;par&quot;, &quot;spon&quot;)))
{% endhighlight %}



{% highlight text %}
## [1] &quot;parity&quot;      &quot;spontaneous&quot;
{% endhighlight %}



{% highlight r %}
dat[, var, with=FALSE]
{% endhighlight %}



{% highlight text %}
##      parity spontaneous
##   1:      6           2
##   2:      1           0
##   3:      6           0
##   4:      4           0
##   5:      3           1
##  ---                   
## 244:      1           1
## 245:      1           0
## 246:      2           0
## 247:      1           1
## 248:      1           1
{% endhighlight %}

Pretty useful, at least for me. You can also use _regular expressions_ to get variables, for instance, something like `lookvar(dat, &quot;p5[0-2]_[a-z]+_2&quot;)`.</content><category term="R" /><summary>Recently, I have been working with big databases. After reading their codebooks (usually very long pdf files), 
I thought it would be useful to have a function to find variable names in R. I wrote a simply function 
that looks for variable names in data.frame and data.table objects.</summary></entry><entry><title>My Facebook Network</title><link href="http://sdaza.com/sna/2012/10/04/facebook-sna/" rel="alternate" type="text/html" title="My Facebook Network" /><published>2012-10-04T00:00:00-05:00</published><updated>2012-10-04T00:00:00-05:00</updated><id>http://sdaza.com/sna/2012/10/04/facebook-sna</id><content type="html" xml:base="http://sdaza.com/sna/2012/10/04/facebook-sna/">I am taking a [coursera](https://www.coursera.org) class on Social Network Analysis. The first assignment consisted of ploting my Facebook network. Below I show how I did it using the R package __igraph__. In order to get your Facebook Network you can use the [Netvizz](https://apps.facebook.com/netvizz/) app. This application allows you to create gdf files (a simple text format that specifies an undirected graph) from the friendship relations. I used this [simplified Netvizz version](http://snacourse.com/getnet/) from [coursera](https://www.coursera.org)  that generates .gml files directly.







{% highlight r %}
library(igraph)

# read the facebook network from a .gml file
f &lt;- read.graph(&quot;sdaza.gml&quot;, format = &quot;gml&quot;)
{% endhighlight %}



In order to plot my Facebook network, I extract the following attributes: gender, wall posts count, and interface language. I also assign some colors and shapes to those attributes. 


{% highlight r %}
# gender, assigning shapes
ssex &lt;- V(f)$sex
table(ssex)
{% endhighlight %}



{% highlight text %}
## ssex
## female   male 
##    218    188
{% endhighlight %}



{% highlight r %}
ssex[ssex == &quot;female&quot;] &lt;- &quot;circle&quot;
ssex[ssex == &quot;male&quot;] &lt;- &quot;square&quot;
table(ssex)
{% endhighlight %}



{% highlight text %}
## ssex
## circle square 
##    218    188
{% endhighlight %}



{% highlight r %}

# interface language, assigning colors
cloc &lt;- V(f)$locale
table(cloc)
{% endhighlight %}



{% highlight text %}
## cloc
## en_GB en_US es_CL es_ES es_LA it_IT nl_NL pt_BR 
##    13   132     2     9   245     1     1     3
{% endhighlight %}



{% highlight r %}

cloc[cloc %in% c(&quot;es_ES&quot;, &quot;it_IT&quot;, &quot;nl_NL&quot;, &quot;en_GB&quot;, &quot;gl_ES&quot;, &quot;ko_KR&quot;)] &lt;-  &quot;Yellow&quot;
cloc[cloc %in% c(&quot;es_CL&quot;, &quot;pt_BR&quot;, &quot;es_LA&quot;)] &lt;- &quot;Green&quot;
cloc[cloc == c(&quot;en_US&quot;)] &lt;- &quot;Blue&quot;
table(cloc)
{% endhighlight %}



{% highlight text %}
## cloc
##   Blue  Green Yellow 
##    132    250     24
{% endhighlight %}



{% highlight r %}

# wall posts count, assigning node size
nsize &lt;- (V(f)$wallcount/max(V(f)$wallcount) + 0.1) * 15
{% endhighlight %}


Now, I can plot the network:


{% highlight r %}
plot(f, layout = layout.fruchterman.reingold, edge.arrow.size = 0.5, 
    vertex.label = NA, vertex.size = 3, vertex.color = cloc, 
    vertex.shape = ssex)
{% endhighlight %}

![center](/img/2012-10-04-facebook-sna/fig1.png) 


If we use __wall posts count__ to size the nodes: 


{% highlight r %}
plot(f, layout = layout.fruchterman.reingold, edge.arrow.size = 0.5, 
    vertex.label = NA, vertex.size = nsize, vertex.color = cloc, 
    vertex.shape = ssex)
{% endhighlight %}

![center](/img/2012-10-04-facebook-sna/fig2.png) 


We can also obtain some basic descriptive statistics of the network using a `graph.basic.stats` function (see [here](http://www.isk.kth.se/~shahabm/WSAnalysis/networks/NetworkAnalysis.r) to obtain it): 


{% highlight r %}
graph.basic.stats(f)
{% endhighlight %}



{% highlight text %}
## Number of nodes: 406 
## Number of edges: 4415 
## 
## Degree
##   Average: 21.7 
## 
## 
## Giant component  Size: 397 
## Giant component  As perc.: 0.978 
## Second Giant component: 1 
## Second Giant component As perc.: 0.00246 
## 
## Isolated nodes
##   Number: 9 
##   As perc.: 0.0222
{% endhighlight %}


As can be seen, I have relatively compact and homogeneous components (or clusters) regarding language interface on Facebook. You can see now how your own Facebook network looks like.</content><category term="SNA" /><summary>I am taking a coursera class on Social Network Analysis. The first assignment consisted of ploting my Facebook network. Below I show how I did it using the R package igraph. In order to get your Facebook Network you can use the Netvizz app. This application allows you to create gdf files (a simple text format that specifies an undirected graph) from the friendship relations. I used this simplified Netvizz version from coursera  that generates .gml files directly.</summary></entry><entry><title>Life tables: Using survey data</title><link href="http://sdaza.com/demography/2012/09/05/demQ01/" rel="alternate" type="text/html" title="Life tables: Using survey data" /><published>2012-09-05T00:00:00-05:00</published><updated>2012-09-05T00:00:00-05:00</updated><id>http://sdaza.com/demography/2012/09/05/demQ01</id><content type="html" xml:base="http://sdaza.com/demography/2012/09/05/demQ01/">Here a life table exercise using fake retrospective survey data:    

&lt;blockquote&gt; You collected retrospective survey data on the age at first birth for twenty-five women who are age 50 at interview in 2009. Construct a life table where the state of interest is childlessness. Start the life table at age 15 and terminate it at exact age 50. Use five year intervals and, for speed, assume that deaths occur halfway through the 5-yr age interval (despite the fact that you have actual ages).Their ages (last birthday) at first birth are 29, 22, 43, 31, 26, 20, no birth, 25, 23, 30, no birth, 37, 21, 25, 28, no birth, 23 (twins), 27, 34, 25, 24, 21, 17, no birth, no birth. On the basis of this information, answer the following questions.&lt;/blockquote&gt;

&lt;h2 class=&quot;section-heading&quot;&gt;What is the probability that a childless woman at exact age 30 would still be childless at age 40?&lt;/h2&gt;

&lt;p&gt;First, we have to build the corresponding life table where the event of interest is childbearing. For childless women during the period I assign 50 years old. In the case of twins it is enough to specify just one event (first birth).&lt;/p&gt;



{% highlight r %}
ages &lt;- c(29, 22, 43, 31, 26, 20, 50, 25, 23, 30, 50, 37, 21,
    25, 28, 50, 23, 27, 34, 25, 24, 21, 17, 50, 50)
# age
x &lt;- seq(15, 49, 5)
# width of intervals
n &lt;- rep(5, 7)
# births
b &lt;- rep(NA, 7)

(dat &lt;- data.frame(x, n, b))
{% endhighlight %}



{% highlight text %}
##    x n  b
## 1 15 5 NA
## 2 20 5 NA
## 3 25 5 NA
## 4 30 5 NA
## 5 35 5 NA
## 6 40 5 NA
## 7 45 5 NA
{% endhighlight %}



{% highlight r %}
# computing births: equivalent to counting logic values
for (i in 1:7) {
    dat$b[i] &lt;- sum(ages &gt;= dat$x[i] &amp; ages &lt;= (dat$x[i + 1] -
        1), na.rm = TRUE)
}

# computing lx
dat$lx &lt;- NA
# childless women at the beginning of the period
dat$lx[1] &lt;- length(ages)

for (i in 1:6) {
    dat$lx[i + 1] &lt;- dat$lx[i] - dat$b[i]
}

dat
{% endhighlight %}



{% highlight text %}
##    x n b lx
## 1 15 5 1 25
## 2 20 5 7 24
## 3 25 5 7 17
## 4 30 5 3 10
## 5 35 5 1  7
## 6 40 5 1  6
## 7 45 5 0  5
{% endhighlight %}


So the probability  a childless woman at exact age 30 would still be childless at age 40 is:


{% highlight r %}
dat$l[dat$x == 40]/dat$l[dat$x == 30]
{% endhighlight %}



{% highlight text %}
## [1] 0.6
{% endhighlight %}


&lt;h2 class=&quot;section-heading&quot;&gt;What was the expected number of years of childlessness (prior to age 50) for a 25 year old childless woman?&lt;/h2&gt;

We have to calculate the equivalent to life expectancy but for childlessness. For that, we need $L_x$ and $T_x$. Following the assumptions specified in the question: _deaths occur halfway through the 5-yr age interval (despite the fact that you have actual ages)_, we can compute readily $L_x$:


{% highlight r %}
# computing Lx assuming nax = interval/2
dat$Lx &lt;- NA
for (i in 1:6) {
    dat$Lx[i] &lt;- dat$lx[i + 1] * dat$n[i] + (dat$n[i]/2) * dat$b[i]
}

# Lx for the last interval
dat$Lx[7] &lt;- 5 * 5

# computing Tx
dat$Tx &lt;- rev(cumsum(rev(dat$Lx)))
dat
{% endhighlight %}



{% highlight text %}
##    x n b lx    Lx    Tx
## 1 15 5 1 25 122.5 420.0
## 2 20 5 7 24 102.5 297.5
## 3 25 5 7 17  67.5 195.0
## 4 30 5 3 10  42.5 127.5
## 5 35 5 1  7  32.5  85.0
## 6 40 5 1  6  27.5  52.5
## 7 45 5 0  5  25.0  25.0
{% endhighlight %}


The expected number of years of childlessness would be $\frac{T_{25}}{l_{25}}$



{% highlight r %}
dat$T[dat$x == 25]/dat$l[dat$x == 25]
{% endhighlight %}



{% highlight text %}
## [1] 11.5
{% endhighlight %}


&lt;h2 class=&quot;section-heading&quot;&gt;What fraction of years between ages 15 and 49.99 were spent childless?&lt;/h2&gt;

That would be $\frac{e_{15}}{35}$:



{% highlight r %}
(dat$T[dat$x == 15]/dat$l[dat$x == 15])/35
{% endhighlight %}



{% highlight text %}
## [1] 0.48
{% endhighlight %}


&lt;h2 class=&quot;section-heading&quot;&gt;You observe that the parity progression ratios for this cohort take the following form. What is the TFR of this cohort?&lt;/h2&gt;

A straightforward way to do it:


{% highlight r %}
# given parity progression ratios
ppr &lt;- c(NA, 0.8, 0.75, 0.25, 0)
# estimation PPR1
# the total number of first births was 20
# the total number of women is 25, so...
(ppr[1] &lt;- (20/25))
{% endhighlight %}



{% highlight text %}
## [1] 0.8
{% endhighlight %}



{% highlight r %}
(TFR &lt;- sum(cumprod(ppr)))
{% endhighlight %}



{% highlight text %}
## [1] 2.04
{% endhighlight %}

&lt;h2 class=&quot;section-heading&quot;&gt;How might your data collection method affect the accuracy of your answer to the former questions? Be specific, referencing the possible direction of bias if applicable.&lt;/h2&gt;


&lt;p&gt;In the survey we are only taking into account surviving women. We will und erestimate the probability of remaining childless in this cohort because single and childless women have higher mortality.&lt;/p&gt;

&lt;h2 class=&quot;section-heading&quot;&gt; Assume that births happened exactly half-way through the year of age in which a woman reported a birth (e.g., a birth reported at age 34 happened at exact age 34.5). How inaccurate is the short-cut estimate of $_{5}a_{20}$ you used above?&lt;/h2&gt;


We have to compute the average number of years lived childless according to the specification for the question.


{% highlight r %}
# ages included in 5a20
(age &lt;- seq(20, 24, 1))
{% endhighlight %}



{% highlight text %}
## [1] 20 21 22 23 24
{% endhighlight %}



{% highlight r %}
# years lived childless according to assumptions of the
# question
(x &lt;- (age - 19.5))
{% endhighlight %}



{% highlight text %}
## [1] 0.5 1.5 2.5 3.5 4.5
{% endhighlight %}



{% highlight r %}
# births between 20 and 24
(y &lt;- table(ages[ages &gt;= 20 &amp; ages &lt;= 24]))
{% endhighlight %}



{% highlight text %}
##
## 20 21 22 23 24
##  1  2  1  2  1
{% endhighlight %}



{% highlight r %}
# average number years lived childless in the interval
mean(x * y)
{% endhighlight %}



{% highlight text %}
## [1] 3.5
{% endhighlight %}


Thus, the short-cut estimate is inaccurate by `3.5 - (5/2) = 1`.</content><category term="demography" /><summary>Here a life table exercise using fake retrospective survey data:</summary></entry><entry><title>Life tables: A super brief introduction</title><link href="http://sdaza.com/demography/2012/08/28/lifetables/" rel="alternate" type="text/html" title="Life tables: A super brief introduction" /><published>2012-08-28T00:00:00-05:00</published><updated>2012-08-28T00:00:00-05:00</updated><id>http://sdaza.com/demography/2012/08/28/lifetables</id><content type="html" xml:base="http://sdaza.com/demography/2012/08/28/lifetables/">A _life table_ is just one way of summarizing a cohort&#39;s mortality experience. Below a simple way to build a life table using R.





{% highlight r %}
dat &lt;- read.csv(&quot;frmales85.csv&quot;, sep = &quot;,&quot;, header = T)
# length of intervals
dat$n &lt;- c(diff(dat$x), NA)
# number of intervals
max &lt;- length(dat$x)
# age 85 is an open interval
dat
{% endhighlight %}



{% highlight text %}
##     x     nNx   nDx   nax  n
## 1   0  379985  3741 0.087  1
## 2   1 1559722   770 1.500  4
## 3   5 1896295   532 2.500  5
## 4  10 2160190   673 2.966  5
## 5  15 2179837  2138 2.769  5
## 6  20 2159556  3432 2.574  5
## 7  25 2106750  3291 2.512  5
## 8  30 2147845  3657 2.586  5
## 9  35 2165387  4956 2.657  5
## 10 40 1516952  5269 2.697  5
## 11 45 1498630  8654 2.695  5
## 12 50 1552746 14490 2.663  5
## 13 55 1476770 20831 2.625  5
## 14 60 1350479 26805 2.601  5
## 15 65  722430 20233 2.615  5
## 16 70  842589 38315 2.598  5
## 17 75  636848 46903 2.538  5
## 18 80  372059 44443 2.466  5
## 19 85  175169 37759 4.639 NA
{% endhighlight %}



{% highlight r %}
# age-specific mortality rates assuming $_nM_x=_nm_x$
dat$nmx &lt;- dat$nDx/dat$nNx
# age-specific probability to die
dat$nqx &lt;- (dat$n * dat$nmx)/(1 + (dat$n - dat$nax) * dat$nmx)
dat$nqx[max] &lt;- 1
dat$npx &lt;- 1 - dat$nqx
{% endhighlight %}



{% highlight r %}
# definition of radix
radix &lt;- 100000
# easy way to get lx without a loop
dat$lx &lt;- radix * c(1, cumprod(dat[1:max - 1, &quot;npx&quot;]))
dat$ndx &lt;- dat$lx * dat$nqx
dat[, c(-2, -3)]
{% endhighlight %}



{% highlight text %}
##     x   nax  n      nmx     nqx   npx     lx   ndx
## 1   0 0.087  1 0.009845 0.00976 0.990 100000   976
## 2   1 1.500  4 0.000494 0.00197 0.998  99024   195
## 3   5 2.500  5 0.000281 0.00140 0.999  98829   139
## 4  10 2.966  5 0.000312 0.00156 0.998  98690   154
## 5  15 2.769  5 0.000981 0.00489 0.995  98537   482
## 6  20 2.574  5 0.001589 0.00792 0.992  98055   776
## 7  25 2.512  5 0.001562 0.00778 0.992  97278   757
## 8  30 2.586  5 0.001703 0.00848 0.992  96522   818
## 9  35 2.657  5 0.002289 0.01138 0.989  95703  1089
## 10 40 2.697  5 0.003473 0.01723 0.983  94614  1630
## 11 45 2.695  5 0.005775 0.02849 0.972  92984  2649
## 12 50 2.663  5 0.009332 0.04566 0.954  90334  4125
## 13 55 2.625  5 0.014106 0.06824 0.932  86209  5883
## 14 60 2.601  5 0.019849 0.09473 0.905  80326  7609
## 15 65 2.615  5 0.028007 0.13127 0.869  72717  9545
## 16 70 2.598  5 0.045473 0.20498 0.795  63171 12949
## 17 75 2.538  5 0.073649 0.31172 0.688  50223 15656
## 18 80 2.466  5 0.119451 0.45848 0.542  34567 15848
## 19 85 4.639 NA 0.215558 1.00000 0.000  18719 18719
{% endhighlight %}



{% highlight r %}
# Lx for the open interval
dat$nLx &lt;- dat$lx * dat$n - (dat$n - dat$nax) * dat$ndx
dat$nLx[max] &lt;- dat$lx[max]/dat$nmx[max]
{% endhighlight %}



{% highlight r %}
dat$Tx &lt;- rev(cumsum(rev(dat$nLx)))
dat$ex &lt;- dat$Tx/dat$lx
dat[, c(-2:-6)]
{% endhighlight %}



{% highlight text %}
##     x     nqx   npx     lx   ndx    nLx      Tx    ex
## 1   0 0.00976 0.990 100000   976  99109 7131028 71.31
## 2   1 0.00197 0.998  99024   195 395609 7031919 71.01
## 3   5 0.00140 0.999  98829   139 493798 6636310 67.15
## 4  10 0.00156 0.998  98690   154 493140 6142512 62.24
## 5  15 0.00489 0.995  98537   482 491608 5649372 57.33
## 6  20 0.00792 0.992  98055   776 488390 5157764 52.60
## 7  25 0.00778 0.992  97278   757 484509 4669374 48.00
## 8  30 0.00848 0.992  96522   818 480632 4184865 43.36
## 9  35 0.01138 0.989  95703  1089 475964 3704232 38.71
## 10 40 0.01723 0.983  94614  1630 469315 3228268 34.12
## 11 45 0.02849 0.972  92984  2649 458812 2758953 29.67
## 12 50 0.04566 0.954  90334  4125 442031 2300141 25.46
## 13 55 0.06824 0.932  86209  5883 417074 1858110 21.55
## 14 60 0.09473 0.905  80326  7609 383376 1441035 17.94
## 15 65 0.13127 0.869  72717  9545 340818 1057660 14.54
## 16 70 0.20498 0.795  63171 12949 284755  716841 11.35
## 17 75 0.31172 0.688  50223 15656 212570  432087  8.60
## 18 80 0.45848 0.542  34567 15848 132677  219516  6.35
## 19 85 1.00000 0.000  18719 18719  86839   86839  4.64
{% endhighlight %}</content><category term="demography" /><summary>A life table is just one way of summarizing a cohorts mortality experience. Below a simple way to build a life table using R.</summary></entry></feed>
